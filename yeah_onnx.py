# -*- coding: utf-8 -*-
"""yeah onnx

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vfK5Dinm-opiYy-Iva83By2-4AKJWXx2
"""

!pip install onnx onnxruntime torch transformers

from google.colab import drive
drive.mount('/content/drive')

import torch

model_path = '/content/drive/MyDrive/malayalam_model_files/balanced_malayalam_model'
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

import onnxruntime
import numpy as np
from transformers import AutoTokenizer

# Load the ONNX model and tokenizer
model_path = '/content/drive/MyDrive/malayalam_model_files/balanced_malayalam_model'
ort_session = onnxruntime.InferenceSession(f'{model_path}/malayalam_model1.onnx')
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)  # Load tokenizer

def predict_with_onnx(text):
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt")

    # Prepare input for ONNX runtime
    ort_inputs = {
        'input_ids': inputs['input_ids'].cpu().numpy(),
        'attention_mask': inputs['attention_mask'].cpu().numpy(),
        'token_type_ids': inputs.get('token_type_ids', torch.zeros_like(inputs['input_ids'])).cpu().numpy()
    }

    # Run inference
    ort_outputs = ort_session.run(None, ort_inputs)

    # Process the output
    logits = ort_outputs[0]
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    predicted_class = np.argmax(probabilities, axis=1)[0]
    predicted_probability = probabilities[0, predicted_class]

    return predicted_class, predicted_probability

# Main loop for taking input and predicting
while True:
    text = input("Enter Malayalam text (or 'q' to quit): ")
    if text.lower() == 'q':
        break

    predicted_class, predicted_probability = predict_with_onnx(text)

    print(f"Predicted Class: {predicted_class}")
    print(f"Predicted Probability: {predicted_probability:.4f}")
    print("-" * 20)